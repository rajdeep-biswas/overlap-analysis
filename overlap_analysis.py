"""
Directory structure -
    tests/
    wordlists/
    -- /
    data/
    -- dataset_csv
    -- label_titles_csv
    -- overlap_scores/
        -- [overlap_11101700_12141700.csv, # overlap_classA_classB
            ...
            ]
    -- label_frequency_csv
    -- unspsc_title_similarity_csv
"""

import logging
import os
import re

import pandas as pd
import spacy

from pandarallel import pandarallel
from spacy_langdetect import LanguageDetector

from tqdm import tqdm

from helper import OverlapAnalysisHelpers



class OverlapAnalysis:

    '''
    Member variables as follows.
    dataset_df: input dataset.
    label_frequency_df: unique UNSPSC labels along with their titles and frequency.
    unspsc_title_similarity_df: pairs of UNSPSC titles and their semantic similarity.

    Note: dataset_df is the only input;
    dataset that will have to be supplied to this code containing LIDs and UNSPSC labels;
    every other dataframe is generated by this code.
    '''

    DATASET_PATH = ''
    LABEL_FREQUENCY_PATH = ''
    UNSPSC_TITLE_SIMILARITY_PATH = ''
    LABEL_TITLES_PATH = ''
    OVERLAP_SCORES_DIR = ''
    OA_RESULT_PATH = ''
    UNIGRAM_COUNT_JSON_PATH = ''

    LOGGING_FORMATTER = ''
    DATE_FORMATTER = ''
    logger = None

    helper = None

    dataset_df = pd.DataFrame()

    label_titles_df = pd.DataFrame()

    title_dfs = {
        'v23_1': pd.DataFrame(),
        'v23_2': pd.DataFrame(),
        'v23_3': pd.DataFrame()
    }

    label_frequency_df = pd.DataFrame(
        columns = [
            'label',
            'label_title',
            'frequency', # optional
            'level' # optional
        ]
    )

    unspsc_title_similarity_df = pd.DataFrame(
        columns = [
            'label_1',
            'label_2',
            'label_1_title',
            'label_2_title',
            'similarity'
        ]
    )

    cross_product_df = pd.DataFrame(
        columns = [
            'lid_1',
            'lid_2',
            'score'
        ]
    )

    similarity_metrics = [
        'averaged_overlap_score'
    ]

    spacy_nlp = None


    def __init__(
        self,
        oa_dir_path: str = '../Data/OA',
        dataset_path: str = 'train_data_initial_invoice_sample.csv',
        label_frequency_path: str = 'label_frequency.csv',
        unspsc_title_similarity_path: str = 'unspsc_title_similarity.csv',
        label_titles_path: str = '!UNSPSC Comparison v905 vs v23.xlsx',
        overlap_scores_dir: str = 'overlap_scores',
        oa_result_path: str = 'overlap_analysis_preprocessing_comparison_results.csv',
        unigram_count_json_path: str = 'unigram_count.json',
        title_sheets: list = ['v23_1', 'v23_2', 'v23_3'],
        selected_labels: list = None
    ):

        '''
        Constructor that takes in root path of data directory within which the input dataset exists.
        Necessary folders will be generated within this directory itself and populated with intermediate and final CSVs.

        dataset_csv_name: filename of the input CSV file relative to the data directory.
        '''

        self.DATASET_PATH = os.path.join(oa_dir_path, dataset_path)
        self.LABEL_FREQUENCY_PATH = os.path.join(oa_dir_path, label_frequency_path)
        self.UNSPSC_TITLE_SIMILARITY_PATH = os.path.join(oa_dir_path, unspsc_title_similarity_path)
        self.LABEL_TITLES_PATH = os.path.join(oa_dir_path, label_titles_path)
        self.OA_RESULT_PATH = os.path.join(oa_dir_path, oa_result_path)
        self.OVERLAP_SCORES_DIR = os.path.join(oa_dir_path, overlap_scores_dir)
        self.UNIGRAM_COUNT_JSON_PATH = os.path.join(oa_dir_path, unigram_count_json_path)

        self.dataset_df = pd.read_csv(self.DATASET_PATH)

        if selected_labels:
            self.dataset_df = self.dataset_df[self.dataset_df.v23_level3.isin(selected_labels)]

        for sheet in title_sheets:
            self.title_dfs[sheet] = pd.read_excel(
                self.LABEL_TITLES_PATH,
                engine = 'openpyxl',
                sheet_name = sheet
            )

        self.helper = OverlapAnalysisHelpers(self.title_dfs)

        tqdm.pandas()

        # pandarallel.initialize(progress_bar=True)

        self.spacy_nlp = spacy.load('en_core_web_sm')
        """
        self.spacy_nlp.add_pipe(
            LanguageDetector(),
            name = 'language_detector',
            last = True
        )
        """

        """ self.LOGGING_FORMATTER =
            '%(asctime)s.%(msecs)03d - %(threadName)s - %(levelname)s - %(message)s'
        self.DATE_FORMATTER = '%Y-%m-%d %H:%M:%S'
        logging.basicConfig(
            format = self.LOGGING_FORMATTER,
            datefmt = self.DATE_FORMATTER,
            level = print
        )
        self.logger = logging.getLogger(__name__) """

        # averaged_overlap_scores, lcs_score, levenshtein_score,
        # cosine_similarity_score, spacy_similarity_score
        """ self.unspsc_title_similarity_df['averaged_overlap_scores'], \
        self.unspsc_title_similarity_df['lcs_score'] \
        self.unspsc_title_similarity_df['levenshtein_score'] \
        ...
            = self.run_overlap_analysis() """



    def generate_label_frequency(self) -> pd.DataFrame: # train_data_initial_invoice

        '''
        Iterates through unique UNSPSC codes from self.dataset_df, ordered by frequency,
        populates labels with their label title into a self.label_frequency_df.
        Also generates a CSV file.
        Possible improvement: maintain a permanent CSV file which records ell UNSPSC labels
        (and their titles ever encountered) - possibly better done with a database.
        '''

        if os.path.isfile(self.LABEL_FREQUENCY_PATH):
            print("label frequency file found.")
            return pd.read_csv(self.LABEL_FREQUENCY_PATH)

        print("label frequency file not found. generating.")

        df_v23_levels = self.dataset_df.v23_level3.value_counts()

        rows = []

        for code in df_v23_levels.keys():
            unspsc_title = self.helper.get_unspsc_label_title(code)

            # determining level. (1 * (level[0] % 2 == 1) is added
            # to handle the corner cases like 94132001.
            level = [m.start() for m in re.finditer(r'(?=(00))', str(code))]
            level = 4 - len(level) + (level[0] % 2 == 1 if len(level) == 1 else len(level) // 2)

            rows.append([code, level, df_v23_levels[code], unspsc_title])

        self.label_frequency_df = pd.DataFrame(data = rows,
                                    columns=['label', 'level', 'count', 'label_title'])

        self.label_frequency_df = self.label_frequency_df[
            self.label_frequency_df.label_title != 'Unknown'
        ]

        self.label_frequency_df.to_csv(self.LABEL_FREQUENCY_PATH)

        print("label frequency file generated and saved as CSV to: " + self.LABEL_FREQUENCY_PATH)

        return self.label_frequency_df



    def overlap_analysis(self) -> int: # oa_2

        '''
        Iterates through pairs of classes from self.unspsc_title_similarity_df,
        and returns a global averaged overlap score,
        which is appended in different columns on self.unspsc_title_similarity_df.
        '''

        def compute_class_overlaps(label_1: int, label_2: int) -> None:

            '''
            Computes each type of similarity score for a `pair` of classes
            as specified in self.similarity_metrics, and returns a dictionary of said score.
            Also, exports these results to a CSV file for each pair of LID (i, j)
            where i is from Class A and j is from Class B.
            '''

            csv_name = 'overlap_' + str(label_1) + '_' + str(label_2) + '.csv'

            if not os.path.exists(self.OVERLAP_SCORES_DIR):
                os.makedirs(self.OVERLAP_SCORES_DIR)

            if os.path.isfile(os.path.join(self.OVERLAP_SCORES_DIR, csv_name)):

                """ print(csv_name + ' already found.') """
                crossed_lid_dfs = pd.read_csv(os.path.join(self.OVERLAP_SCORES_DIR, csv_name))
                return sum(crossed_lid_dfs.averaged_overlap_score) \
                    / len(crossed_lid_dfs.averaged_overlap_score)

            """ print('comparing: ' + str(label_1) + ':' + str(label_2)) """

            df1 = self.dataset_df[self.dataset_df.v23_level3 == label_1]
            df2 = self.dataset_df[self.dataset_df.v23_level3 == label_2]

            # perform cross product on LIDs to generate all possible pairs.
            index = pd.MultiIndex.from_product(
                [df1.lid_concatenated, df2.lid_concatenated],
                names = ["lid_1_reduced", "lid_2_reduced"]
            )
            crossed_lid_dfs = pd.DataFrame(index = index).reset_index()

            # drop (b, a) values when (a, b) already exist
            # crossed_lid_dfs = crossed_lid_dfs.sort_values('lid_1_reduced')
            # crossed_lid_dfs \
            # = crossed_lid_dfs[crossed_lid_dfs['lid_1_reduced'] < crossed_lid_dfs['lid_2_reduced']]

            # get scores and commons comma separated unigrams
            crossed_lid_dfs['averaged_overlap_score'], \
            crossed_lid_dfs['absolute_overlap_score'], \
            crossed_lid_dfs['common_unigrams'] \
                = zip(*crossed_lid_dfs[['lid_1_reduced', 'lid_2_reduced']] \
                    .apply(
                        lambda x: self.helper.unigrams_in_common(*x),
                        axis = 1
                    ).to_numpy())

            # keep track of weighted similarity score before compressing unique rows by count
            similarity_score = sum(crossed_lid_dfs.averaged_overlap_score) \
                / len(crossed_lid_dfs.averaged_overlap_score)

            self.helper.update_unigram_count_json(
                crossed_lid_dfs['common_unigrams'].values,
                self.UNIGRAM_COUNT_JSON_PATH
            )

            crossed_lid_dfs.averaged_overlap_score = crossed_lid_dfs.averaged_overlap_score * 100

            optimize_space = True

            if optimize_space:

                # keep count of duplicate rows to preserve magnitude
                crossed_lid_dfs['count'] = crossed_lid_dfs.groupby([
                    'lid_1_reduced', 'lid_2_reduced'
                ]).transform('count')['averaged_overlap_score']

                # get the count of all rows where there is zero unigram overlap and drop all rows
                count_of_zero = len(crossed_lid_dfs[crossed_lid_dfs.averaged_overlap_score == 0])
                crossed_lid_dfs = crossed_lid_dfs[crossed_lid_dfs.averaged_overlap_score != 0]

                # preserve one single blank row keeping the count of zero-overlap rows
                crossed_lid_dfs.loc[len(crossed_lid_dfs)] = [
                    '<count of LID pairs with zero overlap>',
                    '', 0, 0, '',
                    count_of_zero
                ]
                crossed_lid_dfs = crossed_lid_dfs.drop_duplicates().sort_values(
                    'averaged_overlap_score',
                    ascending = False
                )

            crossed_lid_dfs.to_csv(os.path.join(self.OVERLAP_SCORES_DIR, csv_name))

            return similarity_score


        def compute_class_overlaps_preprocessed(label_1: int, label_2: int) -> None:

            '''
            Computes each type of similarity score for a `pair` of classes
            as specified in self.similarity_metrics, and returns a dictionary of said score.
            Also, exports these results to a CSV file for each pair of LID (i, j)
            where i is from Class A and j is from Class B.
            '''

            csv_name = 'overlap_' + str(label_1) + '_' + str(label_2) + '_preprocessed.csv'

            if not os.path.exists(self.OVERLAP_SCORES_DIR):
                os.makedirs(self.OVERLAP_SCORES_DIR)

            if os.path.isfile(os.path.join(self.OVERLAP_SCORES_DIR, csv_name)):

                """ print(csv_name + ' already found.') """
                crossed_lid_dfs = pd.read_csv(os.path.join(self.OVERLAP_SCORES_DIR, csv_name))
                return sum(crossed_lid_dfs.averaged_overlap_score) \
                    / len(crossed_lid_dfs.averaged_overlap_score)

            """ print('comparing: ' + str(label_1) + ':' + str(label_2)) """

            df1 = self.dataset_df[self.dataset_df.v23_level3 == label_1]
            df2 = self.dataset_df[self.dataset_df.v23_level3 == label_2]

            # perform cross product on LIDs to generate all possible pairs.
            index = pd.MultiIndex.from_product(
                [df1.lid_preprocessed, df2.lid_preprocessed],
                names = ["lid_1_reduced", "lid_2_reduced"]
            )
            crossed_lid_dfs = pd.DataFrame(index = index).reset_index()

            # drop (b, a) values when (a, b) already exist
            # crossed_lid_dfs = crossed_lid_dfs.sort_values('lid_1_reduced')
            # crossed_lid_dfs = \
            # crossed_lid_dfs[crossed_lid_dfs['lid_1_reduced'] < crossed_lid_dfs['lid_2_reduced']]

            # get scores and commons comma separated unigrams
            crossed_lid_dfs['averaged_overlap_score'], \
            crossed_lid_dfs['absolute_overlap_score'], \
            crossed_lid_dfs['common_unigrams'] \
                = zip(*crossed_lid_dfs[['lid_1_reduced', 'lid_2_reduced']] \
                    .apply(
                        lambda x: self.helper.unigrams_in_common(*x),
                        axis = 1
                    ).to_numpy())

            # keep track of weighted similarity score before compressing unique rows by count
            similarity_score = sum(crossed_lid_dfs.averaged_overlap_score) \
                / len(crossed_lid_dfs.averaged_overlap_score)

            self.helper.update_unigram_count_json(
                crossed_lid_dfs['common_unigrams'].values,
                self.UNIGRAM_COUNT_JSON_PATH
            )

            crossed_lid_dfs.averaged_overlap_score = crossed_lid_dfs.averaged_overlap_score * 100

            optimize_space = False

            if optimize_space:

                # keep count of duplicate rows to preserve magnitude
                crossed_lid_dfs['count'] = crossed_lid_dfs.groupby([
                    'lid_1_reduced', 'lid_2_reduced'
                ]).transform('count')['averaged_overlap_score']

                # get the count of all rows where there is zero unigram overlap and drop all rows
                count_of_zero = len(crossed_lid_dfs[crossed_lid_dfs.averaged_overlap_score == 0])
                crossed_lid_dfs = crossed_lid_dfs[crossed_lid_dfs.averaged_overlap_score != 0]

                # preserve one single blank row keeping the count of zero-overlap rows
                crossed_lid_dfs.loc[len(crossed_lid_dfs)] = [
                    '<count of LID pairs with zero overlap>',
                    '', 0, 0, '',
                    count_of_zero
                ]

                crossed_lid_dfs = crossed_lid_dfs.drop_duplicates().sort_values(
                    'averaged_overlap_score',
                    ascending = False
                )

            crossed_lid_dfs.to_csv(os.path.join(self.OVERLAP_SCORES_DIR, csv_name))

            return similarity_score

        print('beginning overlap analysis.')

        self.unspsc_title_similarity_df['similarity'] = self.unspsc_title_similarity_df \
            [['label_1', 'label_2']].progress_apply(lambda x: compute_class_overlaps(*x), axis = 1)

        self.unspsc_title_similarity_df['similarity_preprocessed'] \
            = self.unspsc_title_similarity_df[['label_1', 'label_2']].progress_apply(
                lambda x: compute_class_overlaps_preprocessed(*x),
                axis = 1
            )

        self.unspsc_title_similarity_df.to_csv(self.OA_RESULT_PATH)

        print('overlap analysis concluded. result CSVfile saved to: ' + self.OA_RESULT_PATH)



    def run_overlap_analysis(self):

        '''
        Main method that performs all prerequisite steps to overlap analysis, and OA itself.
        '''

        self.label_frequency_df = self.generate_label_frequency()
        self.unspsc_title_similarity_df = self.helper.generate_interest_pairs(
            self.UNSPSC_TITLE_SIMILARITY_PATH,
            self.label_frequency_df
        )
        self.dataset_df = self.helper.preprocess_lids(self.dataset_df)
        self.overlap_analysis()
